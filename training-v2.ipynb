{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Indonesian Recipe Model Training\n",
                "\n",
                "This notebook implements a fine-tuning process for an Indonesian recipe generation model using Qwen2-7B-Instruct."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "import glob\n",
                "import torch\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from datasets import Dataset, concatenate_datasets\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, \n",
                "    AutoTokenizer, \n",
                "    Trainer, \n",
                "    TrainingArguments, \n",
                "    DataCollatorForLanguageModeling,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from transformers import EarlyStoppingCallback"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration for RTX 3060\n",
                "MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
                "OUTPUT_DIR = \"./indonesian-food-model-final-Qwen-7B-500f\"\n",
                "DATASET_FOLDER = \"./datasets\"\n",
                "\n",
                "# System Message\n",
                "SYSTEM_MESSAGE = \"Kamu adalah Chef Indonesia, asisten AI yang membantu membuat resep masakan Indonesia. Selalu berikan instruksi step by step yang jelas dan terperinci untuk setiap resep yang kamu bagikan. Pastikan langkah-langkah diurutkan dengan baik dan mudah diikuti oleh pengguna.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Loading Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_datasets_from_folder(folder_path, max_recipes=5000):\n",
                "    \"\"\"\n",
                "    Load CSV files with memory-efficient approach\n",
                "    \"\"\"\n",
                "    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
                "    \n",
                "    if not csv_files:\n",
                "        raise ValueError(f\"No CSV files found in the folder: {folder_path}\")\n",
                "    \n",
                "    all_datasets = []\n",
                "    total_recipes_loaded = 0\n",
                "    \n",
                "    for file_path in csv_files:\n",
                "        # Read CSV file in chunks to manage memory\n",
                "        df_chunks = pd.read_csv(file_path, chunksize=1000)\n",
                "        \n",
                "        for chunk in df_chunks:\n",
                "            # Clean and prepare data\n",
                "            chunk = chunk.dropna(subset=['Title', 'Ingredients', 'Steps'])\n",
                "            \n",
                "            def format_recipe(row):\n",
                "                try:\n",
                "                    return {\n",
                "                        \"text\": f\"\"\"<|im_start|>system\n",
                "{SYSTEM_MESSAGE}<|im_end|>\n",
                "<|im_start|>user\n",
                "Tolong jelaskan cara membuat {row['Title']} dengan detail lengkap.<|im_end|>\n",
                "<|im_start|>assistant\n",
                "Nama Resep: {row['Title']}\n",
                "\n",
                "Bahan-Bahan:\n",
                "{row['Ingredients']}\n",
                "\n",
                "Langkah Memasak:\n",
                "{row['Steps']}<|im_end|>\"\"\"\n",
                "                    }\n",
                "                except Exception as e:\n",
                "                    print(f\"Error processing recipe: {e}\")\n",
                "                    return None\n",
                "            \n",
                "            # Filter and format recipes\n",
                "            recipes = chunk.apply(format_recipe, axis=1).dropna().tolist()\n",
                "            \n",
                "            # Convert to Dataset\n",
                "            dataset = Dataset.from_list(recipes)\n",
                "            all_datasets.append(dataset)\n",
                "            \n",
                "            total_recipes_loaded += len(dataset)\n",
                "            \n",
                "            # Stop if max recipes reached\n",
                "            if total_recipes_loaded >= max_recipes:\n",
                "                print(f\"Reached maximum recipes limit: {max_recipes}\")\n",
                "                break\n",
                "        \n",
                "        # Free up memory\n",
                "        del chunk\n",
                "        gc.collect()\n",
                "        \n",
                "        if total_recipes_loaded >= max_recipes:\n",
                "            break\n",
                "    \n",
                "    # Combine datasets\n",
                "    combined_dataset = concatenate_datasets(all_datasets)\n",
                "    print(f\"Total recipes loaded: {len(combined_dataset)}\")\n",
                "    \n",
                "    return combined_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model and Tokenizer Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_model_and_tokenizer():\n",
                "    \"\"\"\n",
                "    Optimized model and tokenizer loading for limited GPU memory\n",
                "    \"\"\"\n",
                "    # Load tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    # Quantization configuration\n",
                "    quantization_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_use_double_quant=True\n",
                "    )\n",
                "    \n",
                "    # Quantization and memory-efficient loading\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME, \n",
                "        device_map='auto',  # Automatic device placement\n",
                "        torch_dtype=torch.float16,\n",
                "        quantization_config=quantization_config\n",
                "    )\n",
                "    \n",
                "    # LoRA configuration optimized for memory\n",
                "    lora_config = LoraConfig(\n",
                "        r=8,  # Reduced rank for memory efficiency\n",
                "        lora_alpha=16,\n",
                "        target_modules=[\"q_proj\", \"v_proj\"],\n",
                "        lora_dropout=0.05,\n",
                "        bias=\"none\",\n",
                "        task_type=\"CAUSAL_LM\"\n",
                "    )\n",
                "    \n",
                "    # Prepare and apply LoRA\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    model = get_peft_model(model, lora_config)\n",
                "    \n",
                "    return model, tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenization Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_dataset(dataset, tokenizer, max_length=512):\n",
                "    \"\"\"\n",
                "    Memory-efficient tokenization\n",
                "    \"\"\"\n",
                "    def tokenize_function(examples):\n",
                "        return tokenizer(\n",
                "            examples['text'], \n",
                "            truncation=True, \n",
                "            padding='max_length', \n",
                "            max_length=max_length\n",
                "        )\n",
                "    \n",
                "    # Use batched processing with smaller batch size\n",
                "    return dataset.map(tokenize_function, batched=True, batch_size=32, remove_columns=dataset.column_names)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Visualization Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def plot_training_performance(trainer):\n",
                "#     \"\"\"\n",
                "#     Visualize training performance metrics\n",
                "#     \"\"\"\n",
                "#     # Extract training logs\n",
                "#     logs = trainer.state.log_history\n",
                "    \n",
                "#     # Prepare data for plotting\n",
                "#     train_losses = [log['loss'] for log in logs if 'loss' in log]\n",
                "#     eval_losses = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
                "    \n",
                "#     # Create figure\n",
                "#     plt.figure(figsize=(12, 6))\n",
                "    \n",
                "#     # Plot training loss\n",
                "#     plt.subplot(1, 2, 1)\n",
                "#     plt.plot(train_losses, label='Training Loss', color='blue')\n",
                "#     plt.title('Training Loss Over Time')\n",
                "#     plt.xlabel('Training Steps')\n",
                "#     plt.ylabel('Loss')\n",
                "#     plt.legend()\n",
                "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
                "    \n",
                "#     # Plot evaluation loss\n",
                "#     plt.subplot(1, 2, 2)\n",
                "#     plt.plot(eval_losses, label='Evaluation Loss', color='green')\n",
                "#     plt.title('Evaluation Loss Over Time')\n",
                "#     plt.xlabel('Evaluation Steps')\n",
                "#     plt.ylabel('Loss')\n",
                "#     plt.legend()\n",
                "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
                "    \n",
                "#     plt.tight_layout()\n",
                "#     plt.show()\n",
                "    \n",
                "#     # Print performance summary\n",
                "#     print(\"\\nTraining Performance Summary:\")\n",
                "#     print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
                "#     print(f\"Final Evaluation Loss: {eval_losses[-1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Training Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def main():\n",
                "    # Clear GPU cache\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()\n",
                "    \n",
                "    # Load datasets\n",
                "    dataset = load_datasets_from_folder(DATASET_FOLDER)\n",
                "    \n",
                "    # Split dataset with stratification\n",
                "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "    \n",
                "    # Prepare model and tokenizer\n",
                "    model, tokenizer = prepare_model_and_tokenizer()\n",
                "    \n",
                "    # Tokenize dataset\n",
                "    tokenized_dataset = tokenize_dataset(dataset['train'], tokenizer)\n",
                "    tokenized_eval_dataset = tokenize_dataset(dataset['test'], tokenizer)\n",
                "    \n",
                "    # Data collator\n",
                "    data_collator = DataCollatorForLanguageModeling(\n",
                "        tokenizer=tokenizer, \n",
                "        mlm=False\n",
                "    )\n",
                "    \n",
                "    # Training arguments optimized for RTX 3060\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        num_train_epochs=3,\n",
                "        per_device_train_batch_size=1,  # Reduced for 12GB VRAM\n",
                "        per_device_eval_batch_size=1,\n",
                "        gradient_accumulation_steps=4,  # Simulate larger batch size\n",
                "        warmup_steps=100,\n",
                "        learning_rate=5e-5,\n",
                "        weight_decay=0.01,\n",
                "        logging_dir='./logs',\n",
                "        logging_steps=10,\n",
                "        evaluation_strategy=\"steps\",\n",
                "        eval_steps=100,\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=100,\n",
                "        load_best_model_at_end=True,\n",
                "        metric_for_best_model=\"eval_loss\",\n",
                "        fp16=True,  # Mixed precision training\n",
                "        max_grad_norm=0.3,  # Gradient clipping\n",
                "    )\n",
                "    \n",
                "    # Initialize Trainer with early stopping\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=tokenized_dataset,\n",
                "        eval_dataset=tokenized_eval_dataset,\n",
                "        data_collator=data_collator,\n",
                "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
                "    )\n",
                "    \n",
                "    # Train the model\n",
                "    trainer.train()\n",
                "    \n",
                "    # Visualize training performance\n",
                "    # plot_training_performance(trainer)\n",
                "    \n",
                "    # Save the model and tokenizer\n",
                "    trainer.save_model(OUTPUT_DIR)\n",
                "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "    \n",
                "    # Clear GPU cache\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()\n",
                "\n",
                "# Run the main training function\n",
                "main()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install transformers datasets torch accelerate peft bitsandbytes pandas matplotlib\n",
                "# !pip install -U \"transformers==4.37.2\" \"datasets==2.16.1\" \"peft==0.7.1\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
